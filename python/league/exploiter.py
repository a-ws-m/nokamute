"""
Exploiter agents with minimax reward shaping.

Implements the minimax reward from the paper:
"Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play"

The minimax reward uses the opponent's value function evaluation to provide
immediate feedback, accelerating convergence compared to sparse game outcomes.
"""

from typing import List, Optional, Tuple

import torch
import torch.nn as nn
from graph_utils import graph_hash
from self_play import SelfPlayGame
from tqdm import tqdm

import nokamute


def compute_minimax_reward(
    current_board: nokamute.Board,
    next_board: nokamute.Board,
    opponent_model: nn.Module,
    device: str = "cpu",
    gamma: float = 0.99,
) -> float:
    """
    Compute the minimax reward for an exploiter action.

    The minimax reward is based on minimizing the opponent's maximum value:
        R_minimax = -gamma * V_opponent(s_{t+1})

    Where:
    - V_opponent(s_{t+1}) is the opponent's value function evaluation
    - gamma is the discount factor
    - The negative sign reflects that we want to minimize the opponent's value

    This provides immediate feedback about how much we've hurt the opponent's
    position, rather than waiting for the final game outcome.

    Args:
        current_board: Board state before action
        next_board: Board state after action
        opponent_model: The opponent's value network (e.g., Main Agent)
        device: Device for model inference
        gamma: Discount factor (default: 0.99)

    Returns:
        Minimax reward value
    """
    opponent_model.eval()

    with torch.no_grad():
        # Convert next state to graph
        node_features, edge_index = next_board.to_graph()

        if len(node_features) == 0:
            # Empty board (shouldn't happen in practice)
            return 0.0

        # Convert to tensors
        x = torch.tensor(node_features, dtype=torch.float32).to(device)
        edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).to(device)

        # Get opponent's evaluation of the position
        # The model outputs values on absolute scale:
        # positive = White winning, negative = Black winning
        opponent_value, _ = opponent_model(x, edge_index_tensor)
        opponent_value = opponent_value.item()

        # Determine who just moved (that's us, the exploiter)
        current_player = (
            current_board.to_move().name
        )  # Player who will move next in current_board
        # After we moved, it's the opponent's turn
        # So we want to minimize the opponent's value

        # If we're White, opponent is Black, so we want opponent_value to be negative
        # If we're Black, opponent is White, so we want opponent_value to be positive
        # Either way, we want to minimize the absolute value from opponent's perspective

        # The opponent will be viewing the position from their perspective next turn
        # Since the model outputs absolute values, we need to consider whose turn it is

        # After our move, it's opponent's turn, and we want them to have low value
        # The minimax reward is: -gamma * V_opponent(s')
        # This means we get positive reward when opponent's position value is negative

        minimax_reward = -gamma * opponent_value

        return minimax_reward


class ExploiterAgent:
    """
    An exploiter agent that uses minimax reward shaping.

    This agent is trained specifically to defeat a target opponent (Main Agent
    or league members) by using the opponent's own value function to guide
    training.
    """

    def __init__(
        self,
        model: nn.Module,
        opponent_model: nn.Module,
        device: str = "cpu",
        temperature: float = 1.0,
        minimax_reward_weight: float = 1.0,
        gamma: float = 0.99,
        enable_branching: bool = False,
        max_moves: int = 400,
        inference_batch_size: Optional[int] = None,
    ):
        """
        Args:
            model: The exploiter's own model (being trained)
            opponent_model: The target opponent's model (frozen)
            device: Device for computation
            temperature: Temperature for move selection
            minimax_reward_weight: Weight for combining minimax reward with game outcome
            gamma: Discount factor for minimax reward
            enable_branching: Enable branching MCMC for game generation
            max_moves: Maximum moves per game
            inference_batch_size: Batch size for position evaluation during inference
        """
        self.model = model
        self.opponent_model = opponent_model
        self.device = device
        self.temperature = temperature
        self.minimax_reward_weight = minimax_reward_weight
        self.gamma = gamma
        self.enable_branching = enable_branching
        self.max_moves = max_moves
        self.inference_batch_size = inference_batch_size

        # Create self-play game generators
        self.exploiter_player = SelfPlayGame(
            model=model,
            temperature=temperature,
            device=device,
            enable_branching=enable_branching,
            max_moves=max_moves,
            inference_batch_size=inference_batch_size,
        )

        self.opponent_player = SelfPlayGame(
            model=opponent_model,
            temperature=0,  # Opponent plays greedily
            device=device,
            enable_branching=False,
            max_moves=max_moves,
            inference_batch_size=inference_batch_size,
        )

    def play_game_with_minimax_rewards(
        self,
        exploiter_is_white: bool = True,
    ) -> Tuple[List, float, str]:
        """
        Play a game between exploiter and opponent, computing minimax rewards.

        Args:
            exploiter_is_white: If True, exploiter plays as White

        Returns:
            game_data: List of (nx_graph, legal_moves, selected_move, player, pos_hash,
                               move_value, minimax_reward)
            result: Final game result (1.0 = White win, -1.0 = Black win, 0.0 = draw)
            branch_id: Branch identifier for tracking
        """
        board = nokamute.Board()
        game_data = []
        branch_id = f"exploiter_game"

        for move_num in range(self.max_moves):
            legal_moves = board.legal_moves()
            winner = board.get_winner()

            if winner is not None:
                # Game over
                if winner == "Draw":
                    result = 0.0
                elif winner == "White":
                    result = 1.0
                else:
                    result = -1.0

                return game_data, result, branch_id

            # Determine which player moves
            current_player = board.to_move()
            is_white_turn = current_player.name == "White"
            is_exploiter_turn = is_white_turn == exploiter_is_white

            # Store current board state
            from graph_utils import board_to_networkx

            node_features, edge_index = board.to_graph()
            nx_graph = board_to_networkx(node_features, edge_index)
            pos_hash = graph_hash(node_features, edge_index)

            # Select move
            if is_exploiter_turn:
                selected_move, move_probs, move_value = (
                    self.exploiter_player.select_move(
                        board, legal_moves, return_probs=True, return_value=True
                    )
                )
            else:
                selected_move, move_probs, move_value = (
                    self.opponent_player.select_move(
                        board, legal_moves, return_probs=True, return_value=True
                    )
                )

            # Apply move
            prev_board = board.clone()
            board.apply(selected_move)

            # Compute minimax reward if it's exploiter's turn
            minimax_reward = 0.0
            if is_exploiter_turn:
                minimax_reward = compute_minimax_reward(
                    prev_board,
                    board,
                    self.opponent_model,
                    device=self.device,
                    gamma=self.gamma,
                )

            # Store data
            game_data.append(
                (
                    nx_graph,
                    legal_moves,
                    selected_move,
                    current_player.name,
                    pos_hash,
                    move_value,
                    minimax_reward,
                )
            )

            del prev_board

        # Max moves reached - draw
        return game_data, 0.0, branch_id

    def generate_training_data(
        self,
        num_games: int,
        exploiter_is_white: bool = True,
    ) -> List[Tuple]:
        """
        Generate training games with minimax rewards.

        Args:
            num_games: Number of games to generate
            exploiter_is_white: If True, exploiter plays White in all games

        Returns:
            List of (game_data, result, branch_id) tuples
        """
        games = []

        for i in tqdm(range(num_games), desc="Generating exploiter games", unit="game"):
            # Alternate colors if desired, or keep fixed
            game_data, result, branch_id = self.play_game_with_minimax_rewards(
                exploiter_is_white=exploiter_is_white
            )
            games.append((game_data, result, branch_id))

        return games


def prepare_exploiter_training_data(
    games: List[Tuple],
    minimax_reward_weight: float = 1.0,
) -> List[Tuple]:
    """
    Prepare training data for exploiter with minimax rewards.

    The target value combines:
    1. Final game outcome (sparse signal)
    2. Minimax rewards (dense signal from opponent's value function)

    Target = (1 - alpha) * game_outcome + alpha * sum(minimax_rewards)

    Where alpha = minimax_reward_weight / (1 + minimax_reward_weight)

    Args:
        games: List of (game_data, result, branch_id) from exploiter games
               game_data contains (..., move_value, minimax_reward)
        minimax_reward_weight: Weight for combining signals (default: 1.0)

    Returns:
        training_examples: List of (node_features, edge_index, target_value)
    """
    from graph_utils import networkx_to_pyg

    position_targets = {}  # pos_hash -> list of target values
    position_data = {}  # pos_hash -> (node_features, edge_index)

    for game_data, final_result, branch_id in tqdm(
        games, desc="Preparing exploiter training data", unit="game"
    ):
        # Compute total minimax reward for this game
        total_minimax_reward = 0.0
        num_exploiter_moves = 0

        for item in game_data:
            if len(item) == 7:
                _, _, _, player, _, _, minimax_reward = item
                # Only count minimax rewards from exploiter's moves
                if minimax_reward != 0.0:
                    total_minimax_reward += minimax_reward
                    num_exploiter_moves += 1

        # Normalize minimax reward by number of moves
        avg_minimax_reward = total_minimax_reward / max(num_exploiter_moves, 1)

        # Combine signals
        alpha = minimax_reward_weight / (1.0 + minimax_reward_weight)
        combined_target = (1 - alpha) * final_result + alpha * avg_minimax_reward

        # Assign to all positions in the game
        for item in game_data:
            if len(item) == 7:
                nx_graph, _, _, player, pos_hash, _, minimax_reward = item

                # Convert NetworkX graph to PyG format
                if pos_hash not in position_data:
                    node_features, edge_index = networkx_to_pyg(nx_graph)

                    if len(node_features) == 0:
                        continue

                    position_data[pos_hash] = (node_features, edge_index)

                # Only train on exploiter's positions
                if minimax_reward != 0.0:
                    if pos_hash not in position_targets:
                        position_targets[pos_hash] = []
                    position_targets[pos_hash].append(combined_target)

    # Average targets for positions seen multiple times
    training_examples = []
    for pos_hash, (node_features, edge_index) in position_data.items():
        if pos_hash in position_targets:
            targets = position_targets[pos_hash]
            avg_target = sum(targets) / len(targets)
            training_examples.append((node_features, edge_index, avg_target))

    return training_examples
