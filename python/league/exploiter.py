"""
Exploiter agents with minimax reward shaping.

Implements the minimax reward from the paper:
"Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play"

The minimax reward uses the opponent's value function evaluation to provide
immediate feedback, accelerating convergence compared to sparse game outcomes.
"""

from typing import List, Optional, Tuple

import torch
import torch.nn as nn
from graph_utils import graph_hash
from self_play import SelfPlayGame
from tqdm import tqdm

import nokamute


def compute_minimax_reward(
    current_board: nokamute.Board,
    next_board: nokamute.Board,
    opponent_model: nn.Module,
    device: str = "cpu",
    gamma: float = 0.99,
) -> float:
    """
    Compute the minimax reward for an exploiter action.

    The minimax reward is based on minimizing the opponent's maximum value:
        R_minimax = -gamma * V_opponent(s_{t+1})

    Where:
    - V_opponent(s_{t+1}) is the opponent's value function evaluation
    - gamma is the discount factor
    - The negative sign reflects that we want to minimize the opponent's value

    This provides immediate feedback about how much we've hurt the opponent's
    position, rather than waiting for the final game outcome.

    Args:
        current_board: Board state before action
        next_board: Board state after action
        opponent_model: The opponent's value network (e.g., Main Agent)
        device: Device for model inference
        gamma: Discount factor (default: 0.99)

    Returns:
        Minimax reward value
    """
    opponent_model.eval()

    with torch.no_grad():
        # Use heterogeneous graph representation
        from hetero_graph_utils import board_to_hetero_data, prepare_model_inputs

        graph_dict = next_board.to_graph()
        data, move_to_action_indices = board_to_hetero_data(graph_dict)

        # Move to device
        data = data.to(device)
        move_to_action_indices = move_to_action_indices.to(device)

        # Prepare inputs
        x_dict, edge_index_dict, edge_attr_dict, move_to_action_indices = (
            prepare_model_inputs(data, move_to_action_indices)
        )

        # Get opponent's evaluation (policy_logits, value)
        outputs = opponent_model(
            x_dict, edge_index_dict, edge_attr_dict, move_to_action_indices
        )

        # The opponent model outputs absolute values: +1 = White winning, -1 = Black winning
        # After our move, it's the opponent's turn to move in next_board
        opponent_color = next_board.to_move().name

        # We want to minimize the opponent's perceived value from their perspective
        # If opponent is White: they want positive values (White winning)
        #   - We want opponent_value to be negative (Black winning)
        #   - Reward = -opponent_value (positive when opponent_value is negative)
        # If opponent is Black: they want negative values (Black winning)
        #   - We want opponent_value to be positive (White winning)
        #   - Reward = opponent_value (positive when opponent_value is positive)

        if opponent_color == "White":
            # Opponent wants positive values, we want to minimize that
            opponent_relative_value = outputs[1]
        else:  # Black
            # Opponent wants negative values, we want to maximize absolute value
            opponent_relative_value = -outputs[2]

        opponent_relative_value = opponent_relative_value.squeeze().item()
        # Minimax reward: positive when we've hurt opponent's position
        # We negate opponent's relative value and apply discount
        minimax_reward = -gamma * opponent_relative_value

        return minimax_reward


PASS_PLACEHOLDER_VALUE = -9999.0


class ExploiterAgent:
    """
    An exploiter agent that uses minimax reward shaping.

    This agent is trained specifically to defeat a target opponent (Main Agent
    or league members) by using the opponent's own value function to guide
    training.
    """

    def __init__(
        self,
        model: nn.Module,
        opponent_model: nn.Module,
        device: str = "cpu",
        epsilon: float = 0.1,
        minimax_reward_weight: float = 1.0,
        gamma: float = 0.99,
        enable_branching: bool = False,
        max_moves: int = 400,
        inference_batch_size: Optional[int] = None,
    ):
        """
        Args:
            model: The exploiter's own model (being trained)
            opponent_model: The target opponent's model (frozen)
            device: Device for computation
            epsilon: Epsilon for epsilon-greedy exploration (exploiter uses this, opponent is greedy)
            minimax_reward_weight: Weight for combining minimax reward with game outcome
            gamma: Discount factor for minimax reward
            enable_branching: Enable branching MCMC for game generation
            max_moves: Maximum moves per game
            inference_batch_size: Batch size for position evaluation during inference
        """
        self.model = model
        self.opponent_model = opponent_model
        self.device = device
        self.epsilon = epsilon
        self.minimax_reward_weight = minimax_reward_weight
        self.gamma = gamma
        self.enable_branching = enable_branching
        self.max_moves = max_moves
        self.inference_batch_size = inference_batch_size

        # Create self-play game generators
        # Exploiter uses epsilon-greedy exploration
        self.exploiter_player = SelfPlayGame(
            model=model,
            epsilon=epsilon,
            device=device,
            enable_branching=enable_branching,
            max_moves=max_moves,
            inference_batch_size=inference_batch_size,
        )

        # Opponent (main agent) plays greedily
        self.opponent_player = SelfPlayGame(
            model=opponent_model,
            epsilon=0.0,
            device=device,
            enable_branching=False,
            max_moves=max_moves,
            inference_batch_size=inference_batch_size,
        )

    def play_game_with_minimax_rewards(
        self,
        exploiter_is_white: bool = True,
    ) -> Tuple[List, float, str]:
        """
        Play a game between exploiter and opponent, computing minimax rewards.

        Args:
            exploiter_is_white: If True, exploiter plays as White

        Returns:
            game_data: List of (hetero_data, move_indices, legal_moves, selected_move, player, pos_hash,
                               move_value, minimax_reward)
            result: Final game result (1.0 = White win, -1.0 = Black win, 0.0 = draw)
            branch_id: Branch identifier for tracking
        """
        board = nokamute.Board()
        game_data = []
        branch_id = f"exploiter_game"

        for move_num in range(self.max_moves):
            legal_moves = board.legal_moves()
            winner = board.get_winner()

            if winner is not None:
                # Game over
                if winner == "Draw":
                    result = 0.0
                elif winner == "White":
                    result = 1.0
                else:
                    result = -1.0

                return game_data, result, branch_id

            # Determine which player moves
            current_player = board.to_move()
            is_white_turn = current_player.name == "White"
            is_exploiter_turn = is_white_turn == exploiter_is_white

            # Store current board state for heterogeneous graph
            # We'll use a simplified representation for hashing
            from hetero_graph_utils import board_to_hetero_data

            graph_dict = board.to_graph()
            data, move_to_action_indices = board_to_hetero_data(graph_dict)

            # Create a simple hash from the heterogeneous graph structure
            # Concatenate all node features for hashing
            all_node_features = []
            if data["in_play"].x.shape[0] > 0:
                all_node_features.append(data["in_play"].x)
            if data["out_of_play"].x.shape[0] > 0:
                all_node_features.append(data["out_of_play"].x)
            if data["destination"].x.shape[0] > 0:
                all_node_features.append(data["destination"].x)

            if all_node_features:
                combined_features = torch.cat(all_node_features, dim=0)
                # Simple hash based on feature values
                pos_hash = hash(combined_features.cpu().numpy().tobytes())
            else:
                pos_hash = 0

            # Store the hetero_data and move_to_action_indices for training
            hetero_data = data  # HeteroData object
            move_indices = move_to_action_indices  # From board_to_hetero_data above

            # Handle pass move
            if (
                len(legal_moves) == 1
                and hasattr(legal_moves[0], "is_pass")
                and legal_moves[0].is_pass()
            ):
                selected_move = legal_moves[0]
                move_probs = {str(selected_move): 1.0}
                move_value = PASS_PLACEHOLDER_VALUE
                minimax_reward = 0.0
                game_data.append(
                    (
                        hetero_data,
                        move_indices,
                        legal_moves,
                        selected_move,
                        current_player.name,
                        pos_hash,
                        move_value,
                        minimax_reward,
                    )
                )
                board.pass_turn()
                continue

            # Select move
            if is_exploiter_turn:
                selected_move, move_probs, move_value = (
                    self.exploiter_player.select_move(
                        board, legal_moves, return_probs=True, return_value=True
                    )
                )
            else:
                selected_move, move_probs, move_value = (
                    self.opponent_player.select_move(
                        board, legal_moves, return_probs=True, return_value=True
                    )
                )

            # Apply move
            prev_board = board.clone()
            board.apply(selected_move)

            # Compute minimax reward if it's exploiter's turn
            minimax_reward = 0.0
            if is_exploiter_turn:
                # Check if opponent has legal moves
                opponent_legal_moves = board.legal_moves()
                if len(opponent_legal_moves) == 0:
                    minimax_reward = 0.0
                else:
                    minimax_reward = compute_minimax_reward(
                        prev_board,
                        board,
                        self.opponent_model,
                        device=self.device,
                        gamma=self.gamma,
                    )

            # Store data
            game_data.append(
                (
                    hetero_data,
                    move_indices,
                    legal_moves,
                    selected_move,
                    current_player.name,
                    pos_hash,
                    move_value,
                    minimax_reward,
                )
            )

            del prev_board

        # Max moves reached - draw
        return game_data, 0.0, branch_id

    def generate_training_data(
        self,
        num_games: int,
        exploiter_is_white: bool = True,
    ) -> List[Tuple]:
        """
        Generate training games with minimax rewards.

        Args:
            num_games: Number of games to generate
            exploiter_is_white: If True, exploiter plays White in all games

        Returns:
            List of (game_data, result, branch_id) tuples
        """
        games = []

        for i in tqdm(range(num_games), desc="Generating exploiter games", unit="game"):
            # Alternate colors if desired, or keep fixed
            game_data, result, branch_id = self.play_game_with_minimax_rewards(
                exploiter_is_white=exploiter_is_white
            )
            games.append((game_data, result, branch_id))

        return games


def prepare_exploiter_training_data(
    games: List[Tuple],
    minimax_reward_weight: float = 1.0,
) -> List[Tuple]:
    """
    Prepare training data for exploiter with minimax rewards.

    The target value combines:
    1. Final game outcome (sparse signal)
    2. Minimax rewards (dense signal from opponent's value function)

    Target = (1 - alpha) * game_outcome + alpha * sum(minimax_rewards)

    Where alpha = minimax_reward_weight / (1 + minimax_reward_weight)

    Args:
        games: List of (game_data, result, branch_id) from exploiter games
               game_data contains (hetero_data, move_indices, legal_moves, selected_move, player, pos_hash, move_value, minimax_reward)
        minimax_reward_weight: Weight for combining signals (default: 1.0)

    Returns:
        training_examples: List of (hetero_data, move_to_action_indices, target_value)
                          where hetero_data is a HeteroData object
    """
    position_targets = {}  # pos_hash -> list of target values
    position_data = {}  # pos_hash -> (HeteroData, move_indices)

    alpha = minimax_reward_weight / (1.0 + minimax_reward_weight)

    for game_data, final_result, branch_id in tqdm(
        games, desc="Preparing exploiter training data", unit="game"
    ):
        # Compute total minimax reward for this game
        total_minimax_reward = 0.0
        num_exploiter_moves = 0

        move_values = []
        for item in game_data:
            if len(item) == 8:
                _, _, _, _, player, _, move_value, minimax_reward = item
                move_values.append(move_value)

        # Replace placeholder values with subsequent move's value
        for i in range(len(move_values)):
            if move_values[i] == PASS_PLACEHOLDER_VALUE:
                for j in range(i + 1, len(move_values)):
                    if move_values[j] != PASS_PLACEHOLDER_VALUE:
                        move_values[i] = move_values[j]
                        break
                else:
                    move_values[i] = final_result

        for idx, item in enumerate(game_data):
            if len(item) == 8:
                (
                    hetero_data,
                    move_indices,
                    _,
                    _,
                    player,
                    pos_hash,
                    move_value,
                    minimax_reward,
                ) = item

                # Store HeteroData object and move_indices
                if pos_hash not in position_data:
                    position_data[pos_hash] = (hetero_data, move_indices)

                # Only train on exploiter's positions
                if minimax_reward != 0.0:
                    if pos_hash not in position_targets:
                        position_targets[pos_hash] = []
                    # Compute combined target
                    combined_target = (
                        1 - alpha
                    ) * final_result + alpha * minimax_reward
                    position_targets[pos_hash].append(combined_target)

    # Average targets for positions seen multiple times
    training_examples = []
    for pos_hash, (hetero_data, move_indices) in position_data.items():
        if pos_hash in position_targets:
            targets = position_targets[pos_hash]
            avg_target = sum(targets) / len(targets)
            # Return HeteroData object with move_indices
            training_examples.append((hetero_data, move_indices, avg_target))

    return training_examples
