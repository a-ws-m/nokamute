"""
Exploiter agents with minimax reward shaping.

Implements the minimax reward from the paper:
"Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play"

The minimax reward uses the opponent's value function evaluation to provide
immediate feedback, accelerating convergence compared to sparse game outcomes.
"""

from typing import List, Optional, Tuple

import torch
import torch.nn as nn
from graph_utils import graph_hash
from self_play import SelfPlayGame
from tqdm import tqdm

import nokamute


def compute_minimax_reward(
    opponent_value: float,
    gamma: float = 0.99,
) -> float:
    """
    Retroactively compute the minimax reward for the exploiter's previous move.

    Args:
        opponent_value: The opponent's value function evaluation after their move
        gamma: Discount factor

    Returns:
        Minimax reward value
    """
    # Minimax reward: positive when we've hurt opponent's position
    # We negate opponent's value and apply discount
    return -gamma * opponent_value


PASS_PLACEHOLDER_VALUE = -9999.0


class ExploiterAgent:
    """
    An exploiter agent that uses minimax reward shaping.

    This agent is trained specifically to defeat a target opponent (Main Agent
    or league members) by using the opponent's own value function to guide
    training.
    """

    def __init__(
        self,
        model: nn.Module,
        opponent_model: nn.Module,
        device: str = "cpu",
        epsilon: float = 0.1,
        minimax_reward_weight: float = 1.0,
        gamma: float = 0.99,
        enable_branching: bool = False,
        max_moves: int = 400,
        inference_batch_size: Optional[int] = None,
    ):
        """
        Args:
            model: The exploiter's own model (being trained)
            opponent_model: The target opponent's model (frozen)
            device: Device for computation
            epsilon: Epsilon for epsilon-greedy exploration (exploiter uses this, opponent is greedy)
            minimax_reward_weight: Weight for combining minimax reward with game outcome
            gamma: Discount factor for minimax reward
            enable_branching: Enable branching MCMC for game generation
            max_moves: Maximum moves per game
            inference_batch_size: Batch size for position evaluation during inference
        """
        self.model = model
        self.opponent_model = opponent_model
        self.device = device
        self.epsilon = epsilon
        self.minimax_reward_weight = minimax_reward_weight
        self.gamma = gamma
        self.enable_branching = enable_branching
        self.max_moves = max_moves
        self.inference_batch_size = inference_batch_size

        # Create self-play game generators
        # Exploiter uses epsilon-greedy exploration
        self.exploiter_player = SelfPlayGame(
            model=model,
            epsilon=epsilon,
            device=device,
            enable_branching=enable_branching,
            max_moves=max_moves,
            inference_batch_size=inference_batch_size,
        )

        # Opponent (main agent) plays greedily
        self.opponent_player = SelfPlayGame(
            model=opponent_model,
            epsilon=0.0,
            device=device,
            enable_branching=False,
            max_moves=max_moves,
            inference_batch_size=inference_batch_size,
        )

    def play_game_with_minimax_rewards(
        self,
        exploiter_is_white: bool = True,
    ) -> Tuple[List, float, str]:
        """
        Play a game between exploiter and opponent, computing minimax rewards retroactively.

        Returns:
            game_data: List of (hetero_data, move_indices, legal_moves, selected_move, player, pos_hash,
                               move_value, minimax_reward)
            result: Final game result (1.0 = White win, -1.0 = Black win, 0.0 = draw)
            branch_id: Branch identifier for tracking
        """
        board = nokamute.Board()
        game_data = []
        branch_id = f"exploiter_game"

        last_exploiter_idx = None
        last_exploiter_player = None

        for move_num in range(self.max_moves):
            legal_moves = board.legal_moves()
            winner = board.get_winner()

            if winner is not None:
                # Game over
                if winner == "Draw":
                    result = 0.0
                elif winner == "White":
                    result = 1.0
                else:
                    result = -1.0

                # If exploiter's last move was not followed by opponent, minimax_reward stays 0
                return game_data, result, branch_id

            current_player = board.to_move()
            is_white_turn = current_player.name == "White"
            is_exploiter_turn = is_white_turn == exploiter_is_white

            from hetero_graph_utils import board_to_hetero_data, prepare_model_inputs

            graph_dict = board.to_graph()
            data, move_to_action_indices = board_to_hetero_data(graph_dict)

            all_node_features = []
            if data["in_play"].x.shape[0] > 0:
                all_node_features.append(data["in_play"].x)
            if data["out_of_play"].x.shape[0] > 0:
                all_node_features.append(data["out_of_play"].x)
            if data["destination"].x.shape[0] > 0:
                all_node_features.append(data["destination"].x)

            if all_node_features:
                combined_features = torch.cat(all_node_features, dim=0)
                pos_hash = hash(combined_features.cpu().numpy().tobytes())
            else:
                pos_hash = 0

            hetero_data = data
            move_indices = move_to_action_indices

            # Handle pass move
            if len(legal_moves) == 1 and legal_moves[0].is_pass():
                selected_move = legal_moves[0]
                move_probs = {str(selected_move): 1.0}
                move_value = PASS_PLACEHOLDER_VALUE
                minimax_reward = 0.0
                game_data.append(
                    (
                        hetero_data,
                        move_indices,
                        legal_moves,
                        selected_move,
                        current_player.name,
                        pos_hash,
                        move_value,
                        minimax_reward,
                    )
                )
                board.pass_turn()
                continue

            # Select move
            if is_exploiter_turn:
                selected_move, move_probs, move_value = (
                    self.exploiter_player.select_move(
                        board, legal_moves, return_probs=True, return_value=True
                    )
                )
            else:
                selected_move, move_probs, move_value = (
                    self.opponent_player.select_move(
                        board, legal_moves, return_probs=True, return_value=True
                    )
                )

            prev_board = board.clone()
            board.apply(selected_move)

            minimax_reward = 0.0

            # Retroactively compute minimax reward for exploiter's previous move
            if not is_exploiter_turn and last_exploiter_idx is not None:
                # Evaluate opponent's value after their move
                graph_dict_next = board.to_graph()
                data_next, move_to_action_indices_next = board_to_hetero_data(
                    graph_dict_next
                )
                data_next = data_next.to(self.device)
                move_to_action_indices_next = move_to_action_indices_next.to(
                    self.device
                )
                x_dict, edge_index_dict, edge_attr_dict, move_to_action_indices_next = (
                    prepare_model_inputs(data_next, move_to_action_indices_next)
                )
                self.opponent_model.eval()
                with torch.no_grad():
                    outputs = self.opponent_model(
                        x_dict,
                        edge_index_dict,
                        edge_attr_dict,
                        move_to_action_indices_next,
                    )
                    # outputs: (action_logits, white_value, black_value, ...)
                    opponent_color = board.to_move().name
                    if opponent_color == "White":
                        opponent_value = outputs[1].squeeze().item()
                    else:
                        opponent_value = -outputs[2].squeeze().item()
                    minimax_reward = compute_minimax_reward(
                        opponent_value, gamma=self.gamma
                    )
                # Update the minimax_reward for exploiter's previous move
                prev_item = list(game_data[last_exploiter_idx])
                prev_item[7] = minimax_reward
                game_data[last_exploiter_idx] = tuple(prev_item)
                last_exploiter_idx = None
                last_exploiter_player = None

            # Store data
            game_data.append(
                (
                    hetero_data,
                    move_indices,
                    legal_moves,
                    selected_move,
                    current_player.name,
                    pos_hash,
                    move_value,
                    minimax_reward,
                )
            )

            # Track exploiter's move index for retroactive reward
            if is_exploiter_turn:
                last_exploiter_idx = len(game_data) - 1
                last_exploiter_player = current_player.name

            del prev_board

        # Max moves reached - draw
        return game_data, 0.0, branch_id

    def generate_training_data(
        self,
        num_games: int,
    ) -> List[Tuple]:
        """
        Generate training games with minimax rewards.

        Args:
            num_games: Number of games to generate

        Returns:
            List of (game_data, result, branch_id) tuples
        """
        games = []

        for i in tqdm(range(num_games), desc="Generating exploiter games", unit="game"):
            # Alternate colors if desired, or keep fixed
            game_data, result, branch_id = self.play_game_with_minimax_rewards(
                exploiter_is_white=(i % 2 == 0)  # Alternate colors
            )
            games.append((game_data, result, branch_id))

        return games


def prepare_exploiter_training_data(
    games: List[Tuple],
    minimax_reward_weight: float = 1.0,
) -> List[Tuple]:
    """
    Prepare training data for exploiter with minimax rewards.

    The target value combines:
    1. Final game outcome (sparse signal)
    2. Minimax rewards (dense signal from opponent's value function)

    Target = (1 - alpha) * game_outcome + alpha * sum(minimax_rewards)

    Where alpha = minimax_reward_weight / (1 + minimax_reward_weight)

    Args:
        games: List of (game_data, result, branch_id) from exploiter games
               game_data contains (hetero_data, move_indices, legal_moves, selected_move, player, pos_hash, move_value, minimax_reward)
        minimax_reward_weight: Weight for combining signals (default: 1.0)

    Returns:
        training_examples: List of (hetero_data, move_to_action_indices, target_value, exploiter_colour)
                          where hetero_data is a HeteroData object
    """
    position_targets = {}  # pos_hash -> list of target values
    position_data = {}  # pos_hash -> (HeteroData, move_indices, exploiter_colour)

    alpha = minimax_reward_weight / (1.0 + minimax_reward_weight)

    for game_data, final_result, branch_id in tqdm(
        games, desc="Preparing exploiter training data", unit="game"
    ):
        # Compute total minimax reward for this game
        total_minimax_reward = 0.0
        num_exploiter_moves = 0

        move_values = []
        for item in game_data:
            if len(item) == 8:
                _, _, _, _, player, _, move_value, minimax_reward = item
                move_values.append(move_value)

        # Replace placeholder values with subsequent move's value
        for i in range(len(move_values)):
            if move_values[i] == PASS_PLACEHOLDER_VALUE:
                for j in range(i + 1, len(move_values)):
                    if move_values[j] != PASS_PLACEHOLDER_VALUE:
                        move_values[i] = move_values[j]
                        break
                else:
                    move_values[i] = final_result

        for idx, item in enumerate(game_data):
            if len(item) == 8:
                (
                    hetero_data,
                    move_indices,
                    _,
                    _,
                    player,
                    pos_hash,
                    move_value,
                    minimax_reward,
                ) = item

                # Store HeteroData object, move_indices, and exploiter_colour
                if pos_hash not in position_data:
                    position_data[pos_hash] = (hetero_data, move_indices, player)

                # Only train on exploiter's positions
                if minimax_reward != 0.0:
                    if pos_hash not in position_targets:
                        position_targets[pos_hash] = []
                    # Compute combined target
                    combined_target = (
                        1 - alpha
                    ) * final_result + alpha * minimax_reward
                    position_targets[pos_hash].append(combined_target)

    # Average targets for positions seen multiple times
    training_examples = []
    for pos_hash, (
        hetero_data,
        move_indices,
        exploiter_colour,
    ) in position_data.items():
        if pos_hash in position_targets:
            targets = position_targets[pos_hash]
            avg_target = sum(targets) / len(targets)
            # Return HeteroData object with move_indices and exploiter_colour
            training_examples.append(
                (hetero_data, move_indices, avg_target, exploiter_colour)
            )

    return training_examples
